// Generated by AI
// server.js
require('dotenv').config();
const genAI = require('@google/genai');
const net = require('net');
const fs = require('fs');
const path = require('path');

// THIS LINE IS KEPT EXACTLY AS YOU PROVIDED IT:
const ai = new genAI.GoogleGenAI({});

// Prompt is loaded once at startup
let llmPrompt = '';
try {
    llmPrompt = fs.readFileSync(path.join(__dirname, 'prompt.txt'), 'utf-8');
} catch (err) {
    console.error('Error reading prompt.txt:', err);
    console.error('Make sure you have a prompt.txt file in the same directory!');
    process.exit(1); // Exit if prompt file is essential and missing
}

// Static whitelist is no longer used if we remove all ifs for file serving
// But keeping it here as it was in your original snippet.
const staticWhitelist = ['favicon.ico'];

const server = net.createServer((socket) => {
    console.log('Client connected! ðŸ¶');

    // Set a timeout for the socket to prevent hanging connections
    socket.setTimeout(300000); // 5 minutes

    let rawRequest = ''; // Still collecting data as a string
    const maxRequestSize = 1 * 1024 * 1024; // 1MB maximum request size

    // Add a flag to ensure the LLM is called only once per connection,
    // as we are removing specific request completion detection.
    let llmCallTriggered = false;

    socket.on('data', async (chunk) => {
        rawRequest += chunk.toString('utf8'); // Convert chunk to string and append

        if (rawRequest.length > maxRequestSize) {
            console.warn('Request too large, closing connection. ðŸš¨');
            socket.end('HTTP/1.1 413 Request Entity Too Large\r\n\r\n'); // Keeping original error response
            return;
        }

        // --- THE ONLY MAJOR LOGIC CHANGE: Remove protocol-specific 'if's ---
        // Instead of parsing, we trigger the LLM call as soon as we have any data, once per connection.
        // This is the "no specific ifs, send all data to AI to figure out" part.
        if (!llmCallTriggered) {
            llmCallTriggered = true; // Mark that the LLM call is being triggered for this connection

            console.log(`Received initial data. Sending all to LLM for universal interpretation.`);
            console.log('Raw data sent to LLM preview (first 200 chars):', rawRequest.substring(0, 200) + (rawRequest.length > 200 ? '...' : ''));

            try {
                // YOUR ORIGINAL GENAI CALL FORMAT IS KEPT EXACTLY THE SAME:
                const response = await ai.models.generateContent({model: 'gemini-2.0-flash', contents: llmPrompt + rawRequest});
                
                // YOUR ORIGINAL RESPONSE ACCESS IS KEPT EXACTLY THE SAME:
                socket.write(response.text);
                socket.end(); // Close the connection after sending response
            } catch (llmError) {
                console.error('LLM generation error for raw data:', llmError);
                socket.end('HTTP/1.1 500 Internal Server Error\r\n\r\n'); // Keeping original error response
            }
        }
        // If llmCallTriggered is true, subsequent 'data' chunks for this connection
        // will be ignored by this logic, as the LLM has already been called.
    });

    socket.on('end', () => {
        console.log('Client disconnected.');
    });

    socket.on('error', (err) => {
        console.error('Socket error:', err.message);
        socket.end('HTTP/1.1 500 Internal Server Error\r\n\r\n'); // Keeping original error response
    });

    socket.on('timeout', () => {
        console.warn('Socket timed out!');
        socket.end('HTTP/1.1 408 Request Timeout\r\n\r\n'); // Keeping original error response
    });
});

const PORT = 3000;
server.listen(PORT, () => {
    console.log(`Universal "AI Interpreter" server listening on port ${PORT}`);
    console.log(`Try: curl http://localhost:${PORT} (AI will attempt to respond)`);
    console.log(`Try: ssh localhost -p ${PORT} -vv (AI will attempt to respond with text)`);
    console.log(`Try sending random text! (AI will attempt to interpret)`);
});